{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Mars News\n",
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.mars_db\n",
    "collection = db.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the parent divs for all paragraphs\n",
    "results = soup.find_all('div', class_='list_text')\n",
    "\n",
    "# loop over results to get article data\n",
    "for result in results[1:]:\n",
    "    # scrape the article title\n",
    "    title = result.find('div', class_='content_title').text\n",
    "    \n",
    "    # scrape the article paragraph\n",
    "    paragraph = result.find('div', class_='article_teaser_body').text\n",
    "    \n",
    "    # scrape the date\n",
    "    date = result.find('div', class_='list_date').text\n",
    "    print(f'paragraph = {paragraph}')\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # print article data\n",
    "    print('-----------------')\n",
    "    print(title)\n",
    "    print(paragraph)\n",
    "    print(date)\n",
    "   \n",
    "\n",
    "    # Dictionary to be inserted into MongoDB\n",
    "    post = {\n",
    "        'title': title,\n",
    "        'paragraph': paragraph,\n",
    "        'date': date\n",
    "    }\n",
    "\n",
    "    # Insert dictionary into MongoDB as a document\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the MongoDB records created above\n",
    "articles = db.articles.find()\n",
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://spaceimages-mars.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featured_image_url = []\n",
    "\n",
    "# Retrieve the parent div for the image\n",
    "results = soup.find_all('div', class_='floating_text_area')\n",
    "\n",
    "# Iterate through the floating text area\n",
    "for result in results:\n",
    "    # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "    a = result.find('a')\n",
    "    link = result.find('a')\n",
    "    href = link['href']\n",
    "    image_url = ('https://spaceimages-mars.com/' + href)\n",
    "    print('-----------')\n",
    "    print(image_url)\n",
    "    featured_image_url.append(image_url)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Click the 'FULL IMAGE' button\n",
    "try:\n",
    "    browser.links.find_by_partial_text('FULL IMAGE').click()\n",
    "          \n",
    "except:\n",
    "    print(\"Scraping Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_image_url[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the url\n",
    "url = 'https://galaxyfacts-mars.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Panda's 'read_html' to parse the url\n",
    "tables = pd.read_html(url)\n",
    "# Find the correct table\n",
    "tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column headings\n",
    "mars_df = tables[1]\n",
    "mars_df.columns = ['Feature', 'Measurement']\n",
    "mars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to an html file\n",
    "mars_df.to_html('mars_data.html', classes = 'table table-striped', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://marshemispheres.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(browser.html, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the parent divs for all images\n",
    "results = soup.find_all('div', class_='item')\n",
    "\n",
    "# loop over results to get image\n",
    "for result in results:\n",
    "    # scrape Valles image\n",
    "    valles = result.find('a', src = 'https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg').text\n",
    "    \n",
    "    # scrape Cerberus image\n",
    "    cerberus = result.find('a', src ='https://marshemispheres.com/images/39d3266553462198bd2fbc4d18fbed17_cerberus_enhanced.tif_thumb.png').text\n",
    "    \n",
    "    # scrape Cerberus image\n",
    "    schiaparelli = result.find('a', src = 'https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg').text\n",
    "                           \n",
    "    # scrape Cerberus image\n",
    "    syrtis = result.find('a', src = 'https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg').text\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # print article data\n",
    "    print('-----------------')\n",
    "    print(title)\n",
    "    print(img_url)\n",
    "   \n",
    "    \n",
    "    hemisphere_image_urls = [\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/b3c7c6c9138f57b4756be9b9c43e3a48_valles_marineris_enhanced.tif_full.jpg\"},\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/f5e372a36edfa389625da6d0cc25d905_cerberus_enhanced.tif_full.jpg\"},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/3778f7b43bbbc89d6e3cfabb3613ba93_schiaparelli_enhanced.tif_full.jpg\"},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/555e6403a6ddd7ba16ddb0e471cadcf7_syrtis_major_enhanced.tif_full.jpg\"},\n",
    "]\n",
    "    \n",
    "    # Insert dictionary into MongoDB as a document\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
